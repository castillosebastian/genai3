{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3c93ac5b",
   "metadata": {},
   "source": [
    "# Running Native Functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "40201641",
   "metadata": {},
   "source": [
    "Two of the previous notebooks showed how to [execute semantic functions inline](./03-semantic-function-inline.ipynb) and how to [run prompts from a file](./02-running-prompts-from-file.ipynb).\n",
    "\n",
    "In this notebook, we'll show how to use native functions from a file. We will also show how to call semantic functions from native functions.\n",
    "\n",
    "This can be useful in a few scenarios:\n",
    "\n",
    "* Writing logic around how to run a prompt that changes the prompt's outcome.\n",
    "* Using external data sources to gather data to concatenate into your prompt.\n",
    "* Validating user input data prior to sending it to the LLM prompt.\n",
    "\n",
    "Native functions are defined using standard Python code. The structure is simple, but not well documented at this point.\n",
    "\n",
    "The following examples are intended to help guide new users towards successful native & semantic function use with the SK Python framework."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d90b0c13",
   "metadata": {},
   "source": [
    "Prepare a semantic kernel instance first, loading also the AI service settings defined in the [Setup notebook](00-getting-started.ipynb):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1da651d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m pip install semantic-kernel==0.4.5.dev0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8c59282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aiofiles==23.2.1\n",
      "aiohttp==3.9.1\n",
      "aiosignal==1.3.1\n",
      "annotated-types==0.6.0\n",
      "anyio==4.2.0\n",
      "asgiref==3.7.2\n",
      "asttokens==2.4.1\n",
      "async-timeout==4.0.3\n",
      "attrs==23.2.0\n",
      "azure-common==1.1.28\n",
      "azure-core==1.29.7\n",
      "azure-identity==1.15.0\n",
      "azure-search-documents==11.4.0\n",
      "certifi==2023.11.17\n",
      "cffi==1.16.0\n",
      "chardet==5.2.0\n",
      "charset-normalizer==3.3.2\n",
      "comm==0.2.1\n",
      "cryptography==41.0.7\n",
      "dataclasses-json==0.6.3\n",
      "debugpy==1.8.0\n",
      "decorator==5.1.1\n",
      "distro==1.9.0\n",
      "dnspython==2.4.2\n",
      "exceptiongroup==1.2.0\n",
      "executing==2.0.1\n",
      "frozenlist==1.4.1\n",
      "greenlet==3.0.3\n",
      "h11==0.14.0\n",
      "httpcore==1.0.2\n",
      "httpx==0.26.0\n",
      "idna==3.6\n",
      "importlib-metadata==7.0.1\n",
      "ipykernel==6.29.0\n",
      "ipython==8.18.1\n",
      "isodate==0.6.1\n",
      "jedi==0.19.1\n",
      "jsonpatch==1.33\n",
      "jsonpointer==2.4\n",
      "jsonschema==4.21.0\n",
      "jsonschema-path==0.3.2\n",
      "jsonschema-spec==0.2.4\n",
      "jsonschema-specifications==2023.7.1\n",
      "jupyter_client==8.6.0\n",
      "jupyter_core==5.7.1\n",
      "langchain==0.1.1\n",
      "langchain-community==0.0.13\n",
      "langchain-core==0.1.13\n",
      "langchain-openai==0.0.3\n",
      "langsmith==0.0.83\n",
      "lazy-object-proxy==1.10.0\n",
      "MarkupSafe==2.1.3\n",
      "marshmallow==3.20.2\n",
      "matplotlib-inline==0.1.6\n",
      "more-itertools==10.2.0\n",
      "motor==3.3.2\n",
      "msal==1.26.0\n",
      "msal-extensions==1.1.0\n",
      "multidict==6.0.4\n",
      "mypy-extensions==1.0.0\n",
      "nest-asyncio==1.5.9\n",
      "numpy==1.26.3\n",
      "openai==1.8.0\n",
      "openapi-core==0.18.2\n",
      "openapi-schema-validator==0.6.2\n",
      "openapi-spec-validator==0.7.1\n",
      "packaging==23.2\n",
      "pandas==2.1.4\n",
      "parse==1.20.0\n",
      "parso==0.8.3\n",
      "pathable==0.4.3\n",
      "pexpect==4.9.0\n",
      "platformdirs==4.1.0\n",
      "portalocker==2.8.2\n",
      "prance==23.6.21.0\n",
      "prompt-toolkit==3.0.43\n",
      "psutil==5.9.7\n",
      "ptyprocess==0.7.0\n",
      "pure-eval==0.2.2\n",
      "pycparser==2.21\n",
      "pydantic==2.5.3\n",
      "pydantic_core==2.14.6\n",
      "Pygments==2.17.2\n",
      "PyJWT==2.8.0\n",
      "pymongo==4.6.1\n",
      "python-dateutil==2.8.2\n",
      "python-dotenv==1.0.0\n",
      "pytz==2023.3.post1\n",
      "PyYAML==6.0.1\n",
      "pyzmq==25.1.2\n",
      "referencing==0.30.2\n",
      "regex==2023.12.25\n",
      "requests==2.31.0\n",
      "rfc3339-validator==0.1.4\n",
      "rpds-py==0.17.1\n",
      "ruamel.yaml==0.18.5\n",
      "ruamel.yaml.clib==0.2.8\n",
      "semantic-kernel==0.4.6.dev0\n",
      "six==1.16.0\n",
      "sniffio==1.3.0\n",
      "SQLAlchemy==2.0.25\n",
      "stack-data==0.6.3\n",
      "tenacity==8.2.3\n",
      "tiktoken==0.5.2\n",
      "tornado==6.4\n",
      "tqdm==4.66.1\n",
      "traitlets==5.14.1\n",
      "typing-inspect==0.9.0\n",
      "typing_extensions==4.9.0\n",
      "tzdata==2023.4\n",
      "urllib3==2.1.0\n",
      "wcwidth==0.2.13\n",
      "Werkzeug==3.0.1\n",
      "yarl==1.9.4\n",
      "zipp==3.17.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ef179ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "endpoint = os.environ[\"AZURE_OPENAI_ENDPOINT\"]\n",
    "deployment = os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"]\n",
    "api_key = os.environ[\"AZURE_OPENAI_API_KEY\"]\n",
    "embeddings = os.environ[\"AZURE_OPENAI_EMBEDDINGS_MODEL_NAME\"]\n",
    "azure_ai_search_api_key = os.environ[\"AZURE_AISEARCH_API_KEY\"]\n",
    "azure_ai_search_url = os.environ[\"AZURE_AISEARCH_ENDPOINT\"]\n",
    "#BING_API_KEY = os.environ[\"BING_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd150646",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<semantic_kernel.kernel.Kernel at 0x7f57aae12c70>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import semantic_kernel as sk\n",
    "from semantic_kernel.connectors.ai.open_ai import (\n",
    "    AzureChatCompletion,\n",
    "    OpenAIChatCompletion,\n",
    ")\n",
    "\n",
    "kernel = sk.Kernel()\n",
    "\n",
    "useAzureOpenAI = True\n",
    "\n",
    "# Configure AI service used by the kernel\n",
    "\n",
    "#deployment, api_key, endpoint = sk.azure_openai_settings_from_dot_env()\n",
    "azure_chat_service = AzureChatCompletion(\n",
    "    deployment_name=deployment, endpoint=endpoint, api_key=api_key\n",
    ")  # set the deployment name to the value of your chat model\n",
    "kernel.add_chat_service(\"chat_completion\", azure_chat_service)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "186767f8",
   "metadata": {},
   "source": [
    "Let's create a **native** function that gives us a random number between 3 and a user input as the upper limit. We'll use this number to create 3-x paragraphs of text when passed to a semantic function."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "589733c5",
   "metadata": {},
   "source": [
    "First, let's create our native function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae29c207",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "#from semantic_kernel.plugin_definition import sk_function\n",
    "from semantic_kernel.plugin_definition.sk_function_decorator import sk_function\n",
    "\n",
    "class GenerateNumberPlugin:\n",
    "    \"\"\"\n",
    "    Description: Generate a number between 3-x.\n",
    "    \"\"\"\n",
    "\n",
    "    @sk_function(\n",
    "        description=\"Generate a random number between 3-x\",\n",
    "        name=\"GenerateNumberThreeOrHigher\",\n",
    "    )\n",
    "    def generate_number_three_or_higher(self, input: str) -> str:\n",
    "        \"\"\"\n",
    "        Generate a number between 3-<input>\n",
    "        Example:\n",
    "            \"8\" => rand(3,8)\n",
    "        Args:\n",
    "            input -- The upper limit for the random number generation\n",
    "        Returns:\n",
    "            int value\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return str(random.randint(3, int(input)))\n",
    "        except ValueError as e:\n",
    "            print(f\"Invalid input {input}\")\n",
    "            raise e"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f26b90c4",
   "metadata": {},
   "source": [
    "Next, let's create a semantic function that accepts a number as `{{$input}}` and generates that number of paragraphs about two Corgis on an adventure. `$input` is a default variable semantic functions can use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7890943f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sk_prompt = \"\"\"\n",
    "Write a short story about two Corgis on an adventure.\n",
    "The story must be:\n",
    "- G rated\n",
    "- Have a positive message\n",
    "- No sexism, racism or other bias/bigotry\n",
    "- Be exactly {{$input}} paragraphs long\n",
    "\"\"\"\n",
    "\n",
    "corgi_story = kernel.create_semantic_function(\n",
    "    prompt_template=sk_prompt,\n",
    "    function_name=\"CorgiStory\",\n",
    "    plugin_name=\"CorgiPlugin\",\n",
    "    description=\"Write a short story about two Corgis on an adventure\",\n",
    "    max_tokens=500,\n",
    "    temperature=0.5,\n",
    "    top_p=0.5,\n",
    ")\n",
    "\n",
    "generate_number_plugin = kernel.import_plugin(GenerateNumberPlugin())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2471c2ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "# Run the number generator\n",
    "generate_number_three_or_higher = generate_number_plugin[\"GenerateNumberThreeOrHigher\"]\n",
    "number_result = generate_number_three_or_higher(6)\n",
    "print(number_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f043a299",
   "metadata": {},
   "outputs": [],
   "source": [
    "story = await corgi_story.invoke_async(input=number_result.result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59a60e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating a corgi story exactly 5 paragraphs long: \n",
      "=====================================================\n",
      "Once upon a time in a cozy little town, there lived two adorable Corgis named Max and Daisy. They were the best of friends and loved going on adventures together. One sunny morning, they decided to explore the nearby forest, eager to discover new sights and smells.\n",
      "\n",
      "As they trotted along the winding path, Max and Daisy marveled at the beauty of nature surrounding them. They saw colorful butterflies fluttering gracefully and heard the cheerful chirping of birds. The forest was alive with vibrant hues and melodious sounds, filling their hearts with joy.\n",
      "\n",
      "Suddenly, they stumbled upon a lost baby bird, chirping sadly. Max and Daisy exchanged worried glances and knew they had to help. With their short legs, they carefully carried the little bird back to its nest, reuniting it with its anxious parents. The grateful birds sang a sweet melody to express their gratitude.\n",
      "\n",
      "Feeling accomplished, Max and Daisy continued their adventure, their spirits soaring high. They encountered a timid squirrel who had lost its acorns. Without hesitation, the kind-hearted Corgis helped gather the scattered acorns, ensuring the squirrel would have enough food for the winter. The squirrel chattered happily, its tiny paws clasped in gratitude.\n",
      "\n",
      "As the sun began to set, Max and Daisy made their way back home, their hearts brimming with happiness. They realized that even small acts of kindness could make a big difference in the lives of others. Their adventure had taught them the importance of compassion, friendship, and lending a helping paw.\n",
      "\n",
      "From that day forward, Max and Daisy became known as the town's beloved heroes. They inspired others to spread kindness and love, reminding everyone that no act of goodness is ever too small. And so, their adventure not only brought joy to their own lives but also touched the hearts of everyone they met, creating a ripple effect of positivity throughout the town.\n"
     ]
    }
   ],
   "source": [
    "print(\"Generating a corgi story exactly {} paragraphs long: \".format(number_result.result))\n",
    "print(\"=====================================================\")\n",
    "print(story)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8ef29d16",
   "metadata": {},
   "source": [
    "## Context Variables\n",
    "\n",
    "That works! But let's expand on our example to make it more generic. \n",
    "\n",
    "For the native function, we'll introduce the lower limit variable. This means that a user will input two numbers and the number generator function will pick a number between the first and second input.\n",
    "\n",
    "We'll make use of the `semantic_kernel.ContextVariables` class to do hold these variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d54983d8",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Azure OpenAI deployment name not found in .env file",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Configure AI service used by the kernel\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m useAzureOpenAI:\n\u001b[0;32m---> 14\u001b[0m     deployment, api_key, endpoint \u001b[38;5;241m=\u001b[39m \u001b[43msk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mazure_openai_settings_from_dot_env\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     azure_chat_service \u001b[38;5;241m=\u001b[39m AzureChatCompletion(\n\u001b[1;32m     16\u001b[0m         deployment_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mturbo\u001b[39m\u001b[38;5;124m\"\u001b[39m, endpoint\u001b[38;5;241m=\u001b[39mendpoint, api_key\u001b[38;5;241m=\u001b[39mapi_key\n\u001b[1;32m     17\u001b[0m     )  \u001b[38;5;66;03m# set the deployment name to the value of your chat model\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     kernel\u001b[38;5;241m.\u001b[39madd_chat_service(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchat_completion\u001b[39m\u001b[38;5;124m\"\u001b[39m, azure_chat_service)\n",
      "File \u001b[0;32m~/.genai3/lib/python3.9/site-packages/semantic_kernel/utils/settings.py:51\u001b[0m, in \u001b[0;36mazure_openai_settings_from_dot_env\u001b[0;34m(include_deployment, include_api_version)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# Azure requires the deployment name, the API key and the endpoint URL.\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_deployment:\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m deployment \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAzure OpenAI deployment name not found in .env file\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_api_version:\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m api_version \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAzure OpenAI API version not found in .env file\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Azure OpenAI deployment name not found in .env file"
     ]
    }
   ],
   "source": [
    "import semantic_kernel as sk\n",
    "from semantic_kernel.connectors.ai.open_ai import (\n",
    "    AzureChatCompletion,\n",
    "    OpenAIChatCompletion,\n",
    ")\n",
    "\n",
    "kernel = sk.Kernel()\n",
    "\n",
    "useAzureOpenAI = True\n",
    "\n",
    "\n",
    "# Configure AI service used by the kernel\n",
    "if useAzureOpenAI:\n",
    "    deployment, api_key, endpoint = sk.azure_openai_settings_from_dot_env()\n",
    "    azure_chat_service = AzureChatCompletion(\n",
    "        deployment_name=\"turbo\", endpoint=endpoint, api_key=api_key\n",
    "    )  # set the deployment name to the value of your chat model\n",
    "    kernel.add_chat_service(\"chat_completion\", azure_chat_service)\n",
    "else:\n",
    "    api_key, org_id = sk.openai_settings_from_dot_env()\n",
    "    oai_chat_service = OpenAIChatCompletion(ai_model_id=\"gpt-3.5-turbo\", api_key=api_key, org_id=org_id)\n",
    "    kernel.add_chat_service(\"chat-gpt\", oai_chat_service)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "091f45e4",
   "metadata": {},
   "source": [
    "Let's start with the native function. Notice that we're also adding `@sk_function_context_parameter` decorators to the function here to provide context about what variables need to be provided to the function, and any defaults for those inputs. Using the `@sk_function_context_parameter` decorator provides the name, description and default values for a function's inputs to the [planner.](./05-using-the-planner.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ea462c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from semantic_kernel.plugin_definition import sk_function, sk_function_context_parameter\n",
    "from semantic_kernel import SKContext\n",
    "\n",
    "\n",
    "class GenerateNumberPlugin:\n",
    "    \"\"\"\n",
    "    Description: Generate a number between a min and a max.\n",
    "    \"\"\"\n",
    "\n",
    "    @sk_function(\n",
    "        description=\"Generate a random number between min and max\",\n",
    "        name=\"GenerateNumber\",\n",
    "    )\n",
    "    @sk_function_context_parameter(name=\"min\", description=\"Minimum number of paragraphs.\")\n",
    "    @sk_function_context_parameter(name=\"max\", description=\"Maximum number of paragraphs.\", default_value=\"10\")\n",
    "    def generate_number(self, context: SKContext) -> str:\n",
    "        \"\"\"\n",
    "        Generate a number between min-max\n",
    "        Example:\n",
    "            min=\"4\" max=\"10\" => rand(4,8)\n",
    "        Args:\n",
    "            min -- The lower limit for the random number generation\n",
    "            max -- The upper limit for the random number generation\n",
    "        Returns:\n",
    "            int value\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return str(random.randint(int(context[\"min\"]), int(context[\"max\"])))\n",
    "        except ValueError as e:\n",
    "            print(f\"Invalid input {context['min']} {context['max']}\")\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "48bcdf9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_number_plugin = kernel.import_plugin(GenerateNumberPlugin())\n",
    "generate_number = generate_number_plugin[\"GenerateNumber\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6ad068d6",
   "metadata": {},
   "source": [
    "Now let's also allow the semantic function to take in additional arguments. In this case, we're going to allow the our CorgiStory function to be written in a specified language. We'll need to provide a `paragraph_count` and a `language`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b8286fb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "TextCompletionClientBase service with service_id 'None' not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 11\u001b[0m\n\u001b[1;32m      1\u001b[0m sk_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124mWrite a short story about two Corgis on an adventure.\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124mThe story must be:\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124m- Be written in this language: \u001b[39m\u001b[38;5;124m{{\u001b[39m\u001b[38;5;124m$language}}\u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m---> 11\u001b[0m corgi_story \u001b[38;5;241m=\u001b[39m \u001b[43mkernel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_semantic_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt_template\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msk_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunction_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCorgiStory\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mplugin_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCorgiPlugin\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWrite a short story about two Corgis on an adventure\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.genai3/lib/python3.9/site-packages/semantic_kernel/kernel.py:794\u001b[0m, in \u001b[0;36mKernel.create_semantic_function\u001b[0;34m(self, prompt_template, function_name, plugin_name, description, **kwargs)\u001b[0m\n\u001b[1;32m    791\u001b[0m template \u001b[38;5;241m=\u001b[39m PromptTemplate(prompt_template, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprompt_template_engine, config)\n\u001b[1;32m    792\u001b[0m function_config \u001b[38;5;241m=\u001b[39m SemanticFunctionConfig(config, template)\n\u001b[0;32m--> 794\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mregister_semantic_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mplugin_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.genai3/lib/python3.9/site-packages/semantic_kernel/kernel.py:121\u001b[0m, in \u001b[0;36mKernel.register_semantic_function\u001b[0;34m(self, plugin_name, function_name, function_config)\u001b[0m\n\u001b[1;32m    118\u001b[0m validate_plugin_name(plugin_name)\n\u001b[1;32m    119\u001b[0m validate_function_name(function_name)\n\u001b[0;32m--> 121\u001b[0m function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_semantic_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mplugin_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_plugin_collection\u001b[38;5;241m.\u001b[39madd_semantic_function(function)\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m function\n",
      "File \u001b[0;32m~/.genai3/lib/python3.9/site-packages/semantic_kernel/kernel.py:673\u001b[0m, in \u001b[0;36mKernel._create_semantic_function\u001b[0;34m(self, plugin_name, function_name, function_config)\u001b[0m\n\u001b[1;32m    671\u001b[0m     function\u001b[38;5;241m.\u001b[39mset_chat_service(\u001b[38;5;28;01mlambda\u001b[39;00m: service(\u001b[38;5;28mself\u001b[39m))\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 673\u001b[0m     service \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_ai_service\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mTextCompletionClientBase\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunction_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprompt_template_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdefault_services\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfunction_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprompt_template_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdefault_services\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\n\u001b[1;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    679\u001b[0m     req_settings_type \u001b[38;5;241m=\u001b[39m service\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__closure__\u001b[39m[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcell_contents\u001b[38;5;241m.\u001b[39mget_request_settings_class()\n\u001b[1;32m    681\u001b[0m     function\u001b[38;5;241m.\u001b[39mset_ai_configuration(\n\u001b[1;32m    682\u001b[0m         req_settings_type\u001b[38;5;241m.\u001b[39mfrom_ai_request_settings(function_config\u001b[38;5;241m.\u001b[39mprompt_template_config\u001b[38;5;241m.\u001b[39mexecution_settings)\n\u001b[1;32m    683\u001b[0m     )\n",
      "File \u001b[0;32m~/.genai3/lib/python3.9/site-packages/semantic_kernel/kernel.py:458\u001b[0m, in \u001b[0;36mKernel.get_ai_service\u001b[0;34m(self, type, service_id)\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown AI service type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m service_id \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m matching_type:\n\u001b[0;32m--> 458\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m service with service_id \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mservice_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not found\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    460\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m matching_type[service_id]\n",
      "\u001b[0;31mValueError\u001b[0m: TextCompletionClientBase service with service_id 'None' not found"
     ]
    }
   ],
   "source": [
    "sk_prompt = \"\"\"\n",
    "Write a short story about two Corgis on an adventure.\n",
    "The story must be:\n",
    "- G rated\n",
    "- Have a positive message\n",
    "- No sexism, racism or other bias/bigotry\n",
    "- Be exactly {{$paragraph_count}} paragraphs long\n",
    "- Be written in this language: {{$language}}\n",
    "\"\"\"\n",
    "\n",
    "corgi_story = kernel.create_semantic_function(\n",
    "    prompt_template=sk_prompt,\n",
    "    function_name=\"CorgiStory\",\n",
    "    plugin_name=\"CorgiPlugin\",\n",
    "    description=\"Write a short story about two Corgis on an adventure\",\n",
    "    max_tokens=500,\n",
    "    temperature=0.5,\n",
    "    top_p=0.5,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fdce1872",
   "metadata": {},
   "source": [
    "Now we can call this using our `invoke` function by passing in our `context_variables` in the `variables` parameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d8d3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_variables = sk.ContextVariables(variables={\"min\": \"1\", \"max\": \"5\", \"language\": \"Spanish\"})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c8778bad",
   "metadata": {},
   "source": [
    "Let's add a paragraph count to our context variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28820d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_variables[\"paragraph_count\"] = generate_number.invoke(variables=context_variables).result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe07c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass the output to the semantic story function\n",
    "story = await corgi_story.invoke_async(variables=context_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6732a30b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\n",
    "    \"Generating a corgi story exactly {} paragraphs long in {} language: \".format(\n",
    "        context_variables[\"paragraph_count\"], context_variables[\"language\"]\n",
    "    )\n",
    ")\n",
    "print(\"=====================================================\")\n",
    "print(story)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fb786c54",
   "metadata": {},
   "source": [
    "## Calling Native Functions within a Semantic Function\n",
    "\n",
    "One neat thing about the Semantic Kernel is that you can also call native functions from within Semantic Functions!\n",
    "\n",
    "We will make our CorgiStory semantic function call a native function `GenerateNames` which will return names for our Corgi characters.\n",
    "\n",
    "We do this using the syntax `{{plugin_name.function_name}}`. You can read more about our prompte templating syntax [here](../../../docs/PROMPT_TEMPLATE_LANGUAGE.md). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84c7d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from semantic_kernel.plugin_definition import sk_function\n",
    "\n",
    "\n",
    "class GenerateNamesPlugin:\n",
    "    \"\"\"\n",
    "    Description: Generate character names.\n",
    "    \"\"\"\n",
    "\n",
    "    # The default function name will be the name of the function itself, however you can override this\n",
    "    # by setting the name=<name override> in the @sk_function decorator. In this case, we're using\n",
    "    # the same name as the function name for simplicity.\n",
    "    @sk_function(description=\"Generate character names\", name=\"generate_names\")\n",
    "    def generate_names(self) -> str:\n",
    "        \"\"\"\n",
    "        Generate two names.\n",
    "        Returns:\n",
    "            str\n",
    "        \"\"\"\n",
    "        names = {\"Hoagie\", \"Hamilton\", \"Bacon\", \"Pizza\", \"Boots\", \"Shorts\", \"Tuna\"}\n",
    "        first_name = random.choice(list(names))\n",
    "        names.remove(first_name)\n",
    "        second_name = random.choice(list(names))\n",
    "        return f\"{first_name}, {second_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab7d65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_names_plugin = kernel.import_plugin(GenerateNamesPlugin(), plugin_name=\"GenerateNames\")\n",
    "generate_names = generate_names_plugin[\"generate_names\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94decd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sk_prompt = \"\"\"\n",
    "Write a short story about two Corgis on an adventure.\n",
    "The story must be:\n",
    "- G rated\n",
    "- Have a positive message\n",
    "- No sexism, racism or other bias/bigotry\n",
    "- Be exactly {{$paragraph_count}} paragraphs long\n",
    "- Be written in this language: {{$language}}\n",
    "- The two names of the corgis are {{GenerateNames.generate_names}}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73aca517",
   "metadata": {},
   "outputs": [],
   "source": [
    "corgi_story = kernel.create_semantic_function(\n",
    "    prompt_template=sk_prompt,\n",
    "    function_name=\"CorgiStory\",\n",
    "    plugin_name=\"CorgiPlugin\",\n",
    "    description=\"Write a short story about two Corgis on an adventure\",\n",
    "    max_tokens=500,\n",
    "    temperature=0.5,\n",
    "    top_p=0.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e6cf0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_variables = sk.ContextVariables(variables={\"min\": \"1\", \"max\": \"5\", \"language\": \"Spanish\"})\n",
    "context_variables[\"paragraph_count\"] = generate_number.invoke(variables=context_variables).result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e980348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass the output to the semantic story function\n",
    "story = await corgi_story.invoke_async(variables=context_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ade048",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"Generating a corgi story exactly {} paragraphs long in {} language: \".format(\n",
    "        context_variables[\"paragraph_count\"], context_variables[\"language\"]\n",
    "    )\n",
    ")\n",
    "print(\"=====================================================\")\n",
    "print(story)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "42f0c472",
   "metadata": {},
   "source": [
    "### Recap\n",
    "\n",
    "A quick review of what we've learned here:\n",
    "\n",
    "- We've learned how to create native and semantic functions and register them to the kernel\n",
    "- We've seen how we can use context variables to pass in more custom variables into our prompt\n",
    "- We've seen how we can call native functions within semantic function prompts.  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
