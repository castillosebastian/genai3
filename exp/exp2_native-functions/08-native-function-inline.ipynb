{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3c93ac5b",
   "metadata": {},
   "source": [
    "# Running Native Functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "40201641",
   "metadata": {},
   "source": [
    "Two of the previous notebooks showed how to [execute semantic functions inline](./03-semantic-function-inline.ipynb) and how to [run prompts from a file](./02-running-prompts-from-file.ipynb).\n",
    "\n",
    "In this notebook, we'll show how to use native functions from a file. We will also show how to call semantic functions from native functions.\n",
    "\n",
    "This can be useful in a few scenarios:\n",
    "\n",
    "* Writing logic around how to run a prompt that changes the prompt's outcome.\n",
    "* Using external data sources to gather data to concatenate into your prompt.\n",
    "* Validating user input data prior to sending it to the LLM prompt.\n",
    "\n",
    "Native functions are defined using standard Python code. The structure is simple, but not well documented at this point.\n",
    "\n",
    "The following examples are intended to help guide new users towards successful native & semantic function use with the SK Python framework."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d90b0c13",
   "metadata": {},
   "source": [
    "Prepare a semantic kernel instance first, loading also the AI service settings defined in the [Setup notebook](00-getting-started.ipynb):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1da651d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m pip install semantic-kernel==0.4.5.dev0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8c59282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aiofiles==23.2.1\n",
      "aiohttp==3.9.1\n",
      "aiosignal==1.3.1\n",
      "annotated-types==0.6.0\n",
      "anyio==4.2.0\n",
      "asgiref==3.7.2\n",
      "asttokens==2.4.1\n",
      "async-timeout==4.0.3\n",
      "attrs==23.2.0\n",
      "azure-common==1.1.28\n",
      "azure-core==1.29.7\n",
      "azure-identity==1.15.0\n",
      "azure-search-documents==11.4.0\n",
      "black==23.12.1\n",
      "certifi==2023.11.17\n",
      "cffi==1.16.0\n",
      "chardet==5.2.0\n",
      "charset-normalizer==3.3.2\n",
      "click==8.1.7\n",
      "comm==0.2.1\n",
      "cryptography==41.0.7\n",
      "dataclasses-json==0.6.3\n",
      "debugpy==1.8.0\n",
      "decorator==5.1.1\n",
      "Deprecated==1.2.14\n",
      "distro==1.9.0\n",
      "dnspython==2.4.2\n",
      "exceptiongroup==1.2.0\n",
      "executing==2.0.1\n",
      "frozenlist==1.4.1\n",
      "fsspec==2023.12.2\n",
      "greenlet==3.0.3\n",
      "h11==0.14.0\n",
      "httpcore==1.0.2\n",
      "httpx==0.26.0\n",
      "idna==3.6\n",
      "importlib-metadata==7.0.1\n",
      "ipykernel==6.29.0\n",
      "ipython==8.18.1\n",
      "isodate==0.6.1\n",
      "jedi==0.19.1\n",
      "joblib==1.3.2\n",
      "jsonpatch==1.33\n",
      "jsonpointer==2.4\n",
      "jsonschema==4.21.0\n",
      "jsonschema-path==0.3.2\n",
      "jsonschema-spec==0.2.4\n",
      "jsonschema-specifications==2023.7.1\n",
      "jupyter_client==8.6.0\n",
      "jupyter_core==5.7.1\n",
      "langchain==0.1.1\n",
      "langchain-community==0.0.13\n",
      "langchain-core==0.1.13\n",
      "langchain-openai==0.0.3\n",
      "langsmith==0.0.83\n",
      "lazy-object-proxy==1.10.0\n",
      "MarkupSafe==2.1.3\n",
      "marshmallow==3.20.2\n",
      "matplotlib-inline==0.1.6\n",
      "more-itertools==10.2.0\n",
      "motor==3.3.2\n",
      "msal==1.26.0\n",
      "msal-extensions==1.1.0\n",
      "multidict==6.0.4\n",
      "mypy-extensions==1.0.0\n",
      "nest-asyncio==1.5.9\n",
      "networkx==3.2.1\n",
      "nltk==3.8.1\n",
      "numpy==1.26.3\n",
      "openai==1.8.0\n",
      "openapi-core==0.18.2\n",
      "openapi-schema-validator==0.6.2\n",
      "openapi-spec-validator==0.7.1\n",
      "packaging==23.2\n",
      "pandas==2.1.4\n",
      "parse==1.20.0\n",
      "parso==0.8.3\n",
      "pathable==0.4.3\n",
      "pathspec==0.12.1\n",
      "pexpect==4.9.0\n",
      "platformdirs==4.1.0\n",
      "portalocker==2.8.2\n",
      "prance==23.6.21.0\n",
      "prompt-toolkit==3.0.43\n",
      "psutil==5.9.7\n",
      "ptyprocess==0.7.0\n",
      "pure-eval==0.2.2\n",
      "pycparser==2.21\n",
      "pydantic==2.5.3\n",
      "pydantic_core==2.14.6\n",
      "Pygments==2.17.2\n",
      "PyJWT==2.8.0\n",
      "pymongo==4.6.1\n",
      "python-dateutil==2.8.2\n",
      "python-dotenv==1.0.0\n",
      "pytz==2023.3.post1\n",
      "PyYAML==6.0.1\n",
      "pyzmq==25.1.2\n",
      "referencing==0.30.2\n",
      "regex==2023.12.25\n",
      "requests==2.31.0\n",
      "rfc3339-validator==0.1.4\n",
      "rpds-py==0.17.1\n",
      "ruamel.yaml==0.18.5\n",
      "ruamel.yaml.clib==0.2.8\n",
      "semantic-kernel==0.4.6.dev0\n",
      "six==1.16.0\n",
      "sniffio==1.3.0\n",
      "SQLAlchemy==2.0.25\n",
      "stack-data==0.6.3\n",
      "tenacity==8.2.3\n",
      "tiktoken==0.5.2\n",
      "tomli==2.0.1\n",
      "tornado==6.4\n",
      "tqdm==4.66.1\n",
      "traitlets==5.14.1\n",
      "typing-inspect==0.9.0\n",
      "typing_extensions==4.9.0\n",
      "tzdata==2023.4\n",
      "urllib3==2.1.0\n",
      "wcwidth==0.2.13\n",
      "Werkzeug==3.0.1\n",
      "wrapt==1.16.0\n",
      "yarl==1.9.4\n",
      "zipp==3.17.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ef179ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "endpoint = os.environ[\"AZURE_OPENAI_ENDPOINT\"]\n",
    "deployment = os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"]\n",
    "api_key = os.environ[\"AZURE_OPENAI_API_KEY\"]\n",
    "embeddings = os.environ[\"AZURE_OPENAI_EMBEDDINGS_MODEL_NAME\"]\n",
    "azure_ai_search_api_key = os.environ[\"AZURE_AISEARCH_API_KEY\"]\n",
    "azure_ai_search_url = os.environ[\"AZURE_AISEARCH_ENDPOINT\"]\n",
    "#BING_API_KEY = os.environ[\"BING_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd150646",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<semantic_kernel.kernel.Kernel at 0x7f0e541467f0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import semantic_kernel as sk\n",
    "from semantic_kernel.connectors.ai.open_ai import (\n",
    "    AzureChatCompletion,\n",
    "    OpenAIChatCompletion,\n",
    ")\n",
    "\n",
    "kernel = sk.Kernel()\n",
    "\n",
    "useAzureOpenAI = True\n",
    "\n",
    "# Configure AI service used by the kernel\n",
    "\n",
    "#deployment, api_key, endpoint = sk.azure_openai_settings_from_dot_env()\n",
    "azure_chat_service = AzureChatCompletion(\n",
    "    deployment_name=deployment, endpoint=endpoint, api_key=api_key\n",
    ")  # set the deployment name to the value of your chat model\n",
    "kernel.add_chat_service(\"chat_completion\", azure_chat_service)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "186767f8",
   "metadata": {},
   "source": [
    "Let's create a **native** function that gives us a random number between 3 and a user input as the upper limit. We'll use this number to create 3-x paragraphs of text when passed to a semantic function."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "589733c5",
   "metadata": {},
   "source": [
    "First, let's create our native function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae29c207",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "#from semantic_kernel.plugin_definition import sk_function\n",
    "from semantic_kernel.plugin_definition.sk_function_decorator import sk_function\n",
    "\n",
    "class GenerateNumberPlugin:\n",
    "    \"\"\"\n",
    "    Description: Generate a number between 3-x.\n",
    "    \"\"\"\n",
    "\n",
    "    @sk_function(\n",
    "        description=\"Generate a random number between 3-x\",\n",
    "        name=\"GenerateNumberThreeOrHigher\",\n",
    "    )\n",
    "    def generate_number_three_or_higher(self, input: str) -> str:\n",
    "        \"\"\"\n",
    "        Generate a number between 3-<input>\n",
    "        Example:\n",
    "            \"8\" => rand(3,8)\n",
    "        Args:\n",
    "            input -- The upper limit for the random number generation\n",
    "        Returns:\n",
    "            int value\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return str(random.randint(3, int(input)))\n",
    "        except ValueError as e:\n",
    "            print(f\"Invalid input {input}\")\n",
    "            raise e"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f26b90c4",
   "metadata": {},
   "source": [
    "Next, let's create a semantic function that accepts a number as `{{$input}}` and generates that number of paragraphs about two Corgis on an adventure. `$input` is a default variable semantic functions can use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7890943f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sk_prompt = \"\"\"\n",
    "Write a short story about two Corgis on an adventure.\n",
    "The story must be:\n",
    "- G rated\n",
    "- Have a positive message\n",
    "- No sexism, racism or other bias/bigotry\n",
    "- Be exactly {{$input}} paragraphs long\n",
    "\"\"\"\n",
    "\n",
    "corgi_story = kernel.create_semantic_function(\n",
    "    prompt_template=sk_prompt,\n",
    "    function_name=\"CorgiStory\",\n",
    "    plugin_name=\"CorgiPlugin\",\n",
    "    description=\"Write a short story about two Corgis on an adventure\",\n",
    "    max_tokens=500,\n",
    "    temperature=0.5,\n",
    "    top_p=0.5,\n",
    ")\n",
    "\n",
    "generate_number_plugin = kernel.import_plugin(GenerateNumberPlugin())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2471c2ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "# Run the number generator\n",
    "generate_number_three_or_higher = generate_number_plugin[\"GenerateNumberThreeOrHigher\"]\n",
    "number_result = generate_number_three_or_higher(6)\n",
    "print(number_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f043a299",
   "metadata": {},
   "outputs": [],
   "source": [
    "story = await corgi_story.invoke_async(input=number_result.result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59a60e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating a corgi story exactly 3 paragraphs long: \n",
      "=====================================================\n",
      "Once upon a time in a quaint little town, two adorable Corgis named Charlie and Daisy embarked on an exciting adventure. Their wagging tails and curious eyes were filled with anticipation as they set off to explore the nearby forest. As they trotted along the winding path, they encountered various creatures and breathtaking sights. Charlie and Daisy greeted each one with kindness and respect, making new friends along the way.\n",
      "\n",
      "Their adventure took an unexpected turn when they stumbled upon a lost baby bird. It chirped helplessly, unable to find its way back home. Without hesitation, Charlie and Daisy worked together to build a cozy nest and gently placed the bird inside. They took turns keeping it warm and fed until its family returned. The grateful bird family sang a melodious tune to express their gratitude, filling the forest with joy.\n",
      "\n",
      "As the sun began to set, Charlie and Daisy returned home, their hearts filled with contentment. They realized that kindness and compassion had made their adventure truly special. From that day forward, they continued to spread love and positivity wherever they went, inspiring others to do the same. And so, their tale of friendship and goodwill became a legend in the town, reminding everyone that even the smallest acts of kindness can create a world of happiness.\n"
     ]
    }
   ],
   "source": [
    "print(\"Generating a corgi story exactly {} paragraphs long: \".format(number_result.result))\n",
    "print(\"=====================================================\")\n",
    "print(story)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8ef29d16",
   "metadata": {},
   "source": [
    "## Context Variables\n",
    "\n",
    "That works! But let's expand on our example to make it more generic. \n",
    "\n",
    "For the native function, we'll introduce the lower limit variable. This means that a user will input two numbers and the number generator function will pick a number between the first and second input.\n",
    "\n",
    "We'll make use of the `semantic_kernel.ContextVariables` class to do hold these variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d54983d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import semantic_kernel as sk\n",
    "# from semantic_kernel.connectors.ai.open_ai import (\n",
    "#     AzureChatCompletion,\n",
    "#     OpenAIChatCompletion,\n",
    "# )\n",
    "\n",
    "# kernel = sk.Kernel()\n",
    "\n",
    "# useAzureOpenAI = True\n",
    "\n",
    "\n",
    "# # Configure AI service used by the kernel\n",
    "\n",
    "# deployment, api_key, endpoint = sk.azure_openai_settings_from_dot_env()\n",
    "# azure_chat_service = AzureChatCompletion(\n",
    "#         deployment_name=\"turbo\", endpoint=endpoint, api_key=api_key\n",
    "#     )  # set the deployment name to the value of your chat model\n",
    "# kernel.add_chat_service(\"chat_completion\", azure_chat_service)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "091f45e4",
   "metadata": {},
   "source": [
    "Let's start with the native function. Notice that we're also adding `@sk_function_context_parameter` decorators to the function here to provide context about what variables need to be provided to the function, and any defaults for those inputs. Using the `@sk_function_context_parameter` decorator provides the name, description and default values for a function's inputs to the [planner.](./05-using-the-planner.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ea462c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from semantic_kernel.plugin_definition import sk_function, sk_function_context_parameter\n",
    "from semantic_kernel import SKContext\n",
    "\n",
    "\n",
    "class GenerateNumberPlugin:\n",
    "    \"\"\"\n",
    "    Description: Generate a number between a min and a max.\n",
    "    \"\"\"\n",
    "\n",
    "    @sk_function(\n",
    "        description=\"Generate a random number between min and max\",\n",
    "        name=\"GenerateNumber\",\n",
    "    )\n",
    "    @sk_function_context_parameter(name=\"min\", description=\"Minimum number of paragraphs.\")\n",
    "    @sk_function_context_parameter(name=\"max\", description=\"Maximum number of paragraphs.\", default_value=\"10\")\n",
    "    def generate_number(self, context: SKContext) -> str:\n",
    "        \"\"\"\n",
    "        Generate a number between min-max\n",
    "        Example:\n",
    "            min=\"4\" max=\"10\" => rand(4,8)\n",
    "        Args:\n",
    "            min -- The lower limit for the random number generation\n",
    "            max -- The upper limit for the random number generation\n",
    "        Returns:\n",
    "            int value\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return str(random.randint(int(context[\"min\"]), int(context[\"max\"])))\n",
    "        except ValueError as e:\n",
    "            print(f\"Invalid input {context['min']} {context['max']}\")\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "48bcdf9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_number_plugin = kernel.import_plugin(GenerateNumberPlugin())\n",
    "generate_number = generate_number_plugin[\"GenerateNumber\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6ad068d6",
   "metadata": {},
   "source": [
    "Now let's also allow the semantic function to take in additional arguments. In this case, we're going to allow the our CorgiStory function to be written in a specified language. We'll need to provide a `paragraph_count` and a `language`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8b8286fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sk_prompt = \"\"\"\n",
    "Write a short story about two Corgis on an adventure.\n",
    "The story must be:\n",
    "- G rated\n",
    "- Have a positive message\n",
    "- No sexism, racism or other bias/bigotry\n",
    "- Be exactly {{$paragraph_count}} paragraphs long\n",
    "- Be written in this language: {{$language}}\n",
    "\"\"\"\n",
    "\n",
    "corgi_story = kernel.create_semantic_function(\n",
    "    prompt_template=sk_prompt,\n",
    "    function_name=\"CorgiStory\",\n",
    "    plugin_name=\"CorgiPlugin\",\n",
    "    description=\"Write a short story about two Corgis on an adventure\",\n",
    "    max_tokens=500,\n",
    "    temperature=0.5,\n",
    "    top_p=0.5,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fdce1872",
   "metadata": {},
   "source": [
    "Now we can call this using our `invoke` function by passing in our `context_variables` in the `variables` parameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "97d8d3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_variables = sk.ContextVariables(variables={\"min\": \"1\", \"max\": \"5\", \"language\": \"Spanish\"})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c8778bad",
   "metadata": {},
   "source": [
    "Let's add a paragraph count to our context variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "28820d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_variables[\"paragraph_count\"] = generate_number.invoke(variables=context_variables).result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dbe07c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass the output to the semantic story function\n",
    "story = await corgi_story.invoke_async(variables=context_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6732a30b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating a corgi story exactly 1 paragraphs long in Spanish language: \n",
      "=====================================================\n",
      "Había una vez dos corgis llamados Max y Lola, quienes vivían en un pequeño pueblo rodeado de hermosos paisajes. Un día, decidieron embarcarse en una aventura para explorar el mundo más allá de su hogar. Juntos, caminaron por senderos desconocidos, saltaron sobre rocas y se maravillaron con la naturaleza que los rodeaba. Durante su viaje, conocieron a otros animales amigables y compartieron risas y juegos. A medida que avanzaban, Max y Lola se dieron cuenta de que la verdadera belleza de la vida radicaba en la amistad y la conexión con los demás. Al final de su aventura, regresaron a casa con corazones llenos de gratitud y una lección aprendida: la felicidad se encuentra en los momentos compartidos y en la apreciación de las pequeñas cosas. Desde entonces, Max y Lola siempre estuvieron dispuestos a embarcarse en nuevas aventuras, sabiendo que el mundo estaba lleno de maravillas por descubrir y amigos por hacer.\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"Generating a corgi story exactly {} paragraphs long in {} language: \".format(\n",
    "        context_variables[\"paragraph_count\"], context_variables[\"language\"]\n",
    "    )\n",
    ")\n",
    "print(\"=====================================================\")\n",
    "print(story)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fb786c54",
   "metadata": {},
   "source": [
    "## Calling Native Functions within a Semantic Function\n",
    "\n",
    "One neat thing about the Semantic Kernel is that you can also call native functions from within Semantic Functions!\n",
    "\n",
    "We will make our CorgiStory semantic function call a native function `GenerateNames` which will return names for our Corgi characters.\n",
    "\n",
    "We do this using the syntax `{{plugin_name.function_name}}`. You can read more about our prompte templating syntax [here](../../../docs/PROMPT_TEMPLATE_LANGUAGE.md). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d84c7d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from semantic_kernel.plugin_definition import sk_function\n",
    "\n",
    "\n",
    "class GenerateNamesPlugin:\n",
    "    \"\"\"\n",
    "    Description: Generate character names.\n",
    "    \"\"\"\n",
    "\n",
    "    # The default function name will be the name of the function itself, however you can override this\n",
    "    # by setting the name=<name override> in the @sk_function decorator. In this case, we're using\n",
    "    # the same name as the function name for simplicity.\n",
    "    @sk_function(description=\"Generate character names\", name=\"generate_names\")\n",
    "    def generate_names(self) -> str:\n",
    "        \"\"\"\n",
    "        Generate two names.\n",
    "        Returns:\n",
    "            str\n",
    "        \"\"\"\n",
    "        names = {\"Hoagie\", \"Hamilton\", \"Bacon\", \"Pizza\", \"Boots\", \"Shorts\", \"Tuna\"}\n",
    "        first_name = random.choice(list(names))\n",
    "        names.remove(first_name)\n",
    "        second_name = random.choice(list(names))\n",
    "        return f\"{first_name}, {second_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2ab7d65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_names_plugin = kernel.import_plugin(GenerateNamesPlugin(), plugin_name=\"GenerateNames\")\n",
    "generate_names = generate_names_plugin[\"generate_names\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "94decd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sk_prompt = \"\"\"\n",
    "Write a short story about two Corgis on an adventure.\n",
    "The story must be:\n",
    "- G rated\n",
    "- Have a positive message\n",
    "- No sexism, racism or other bias/bigotry\n",
    "- Be exactly {{$paragraph_count}} paragraphs long\n",
    "- Be written in this language: {{$language}}\n",
    "- The two names of the corgis are {{GenerateNames.generate_names}}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "73aca517",
   "metadata": {},
   "outputs": [],
   "source": [
    "corgi_story = kernel.create_semantic_function(\n",
    "    prompt_template=sk_prompt,\n",
    "    function_name=\"CorgiStory\",\n",
    "    plugin_name=\"CorgiPlugin\",\n",
    "    description=\"Write a short story about two Corgis on an adventure\",\n",
    "    max_tokens=500,\n",
    "    temperature=0.5,\n",
    "    top_p=0.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "56e6cf0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_variables = sk.ContextVariables(variables={\"min\": \"1\", \"max\": \"5\", \"language\": \"Spanish\"})\n",
    "context_variables[\"paragraph_count\"] = generate_number.invoke(variables=context_variables).result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7e980348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass the output to the semantic story function\n",
    "story = await corgi_story.invoke_async(variables=context_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c4ade048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating a corgi story exactly 2 paragraphs long in Spanish language: \n",
      "=====================================================\n",
      "Había una vez dos corgis llamados Hoagie y Bacon que vivían en un pequeño pueblo. Eran los mejores amigos y siempre estaban buscando aventuras juntos. Un día, decidieron explorar el bosque mágico que se encontraba al otro lado del río.\n",
      "\n",
      "Mientras caminaban por el bosque, Hoagie y Bacon se encontraron con un conejito asustado. El conejito les contó que se había perdido y no podía encontrar su camino de regreso a casa. Los corgis, llenos de compasión, decidieron ayudar al conejito. Juntos, buscaron pistas y siguieron el rastro de las hojas caídas hasta que finalmente encontraron la madriguera del conejito. El conejito estaba tan agradecido que les prometió ser su amigo para siempre.\n",
      "\n",
      "Hoagie y Bacon aprendieron que siempre es importante ayudar a los demás, sin importar cuán pequeños o grandes sean. Su amistad y espíritu de aventura les permitieron hacer nuevos amigos y descubrir la importancia de la bondad y la compasión. Desde ese día en adelante, Hoagie, Bacon y el conejito se convirtieron en los mejores amigos y juntos vivieron muchas más aventuras en el bosque mágico.\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"Generating a corgi story exactly {} paragraphs long in {} language: \".format(\n",
    "        context_variables[\"paragraph_count\"], context_variables[\"language\"]\n",
    "    )\n",
    ")\n",
    "print(\"=====================================================\")\n",
    "print(story)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "42f0c472",
   "metadata": {},
   "source": [
    "### Recap\n",
    "\n",
    "A quick review of what we've learned here:\n",
    "\n",
    "- We've learned how to create native and semantic functions and register them to the kernel\n",
    "- We've seen how we can use context variables to pass in more custom variables into our prompt\n",
    "- We've seen how we can call native functions within semantic function prompts.  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
